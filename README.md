# ğŸš• Desafio TÃ©cnico: Analytics Engineer â€” NYC Taxi Trip Data

## ğŸ“Œ VisÃ£o Geral do Projeto
Este repositÃ³rio apresenta a soluÃ§Ã£o desenvolvida para o desafio tÃ©cnico de Engenheiro(a) de Dados Pleno, com foco na ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise de dados de corridas de tÃ¡xis da cidade de Nova York.

A proposta demonstra habilidades em:
- Engenharia de Dados/Software
- AnÃ¡lise e Modelagem de Dados
- ConstruÃ§Ã£o de um pipeline robusto desde os dados brutos atÃ© sua disponibilizaÃ§Ã£o para consumo e geraÃ§Ã£o de insights

## ğŸ¯ Objetivo do Desafio
O desafio consistiu em:
- Ingerir dados de corridas de tÃ¡xis de NYC em um Data Lake
- Disponibilizar os dados para consumo via SQL
- Realizar anÃ¡lises especÃ­ficas

ğŸ—“ Os dados utilizados referem-se Ã s corridas de Yellow Taxis, Green Taxis, FHV e HVFHV entre janeiro e maio de 2023, obtidos do [site oficial da TLC](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

## ğŸ— Arquitetura da SoluÃ§Ã£o
A arquitetura adotada segue o padrÃ£o *MedalhÃ£o* (Bronze/Bronze, Silver/Curated) para garantir qualidade e governanÃ§a dos dados.

- **Armazenamento**: Amazon S3 como Data Lake principal, com buckets separados para as camadas *bronze* e *silver*
- **Processamento**: Databricks Community Edition com PySpark
- **Formato dos dados**: Delta Lake (transaÃ§Ãµes ACID, schema enforcement/evolution, time travel)
- **Metadados**: Hive Metastore integrado ao Databricks
- **Consumo**: SQL via Databricks SQL Endpoints e PySpark para anÃ¡lises avanÃ§adas

## ğŸ›  Tecnologias Utilizadas
- **Linguagens**: Python (PySpark), SQL  
- **Plataforma de Dados**: Databricks Community Edition  
- **Armazenamento em Nuvem**: Amazon S3  
- **Formato de Dados**: Delta Lake  
- **Versionamento de CÃ³digo**: Git  

## âš™ï¸ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o â€” Passos de Alto NÃ­vel

1. **ConfiguraÃ§Ã£o AWS S3**
   - Crie dois buckets na mesma regiÃ£o:
     - `ifood-case-nyc-taxi-data-lake-raw`
     - `ifood-case-nyc-taxi-data-lake-silver`
   - Habilite o bloqueio de acesso pÃºblico e o versionamento

2. **Download e Upload dos Dados**
   - Baixe arquivos PARQUET dos tipos: Yellow, Green, FHV e HVFHV (Jan-Mai/2023)
   - Estruture no S3 conforme:  
     `nyctlc/<trip_type>/parquet/year=<yyyy>/month=<mm>/`

3. **Clonagem do RepositÃ³rio no Databricks**
   - `Repos â†’ Add Repo â†’ Git URL` do seu fork
   - Caminho final: `Workspace/Repos/<usuÃ¡rio>/ifood-case`

4. **Ajustes para ExecuÃ§Ã£o**
   - No arquivo `src/config.py`, defina o nome do database na linha 16:
     <img width="626" height="60" alt="image" src="https://github.com/user-attachments/assets/00c425d9-da5b-4c74-841c-1c38b2172cd5" />

   - No notebook `Driver`, insira o caminho do repositÃ³rio em `repo_path`:
     <img width="886" height="136" alt="image" src="https://github.com/user-attachments/assets/dd4317b7-44af-440d-bbd2-69152fe1ebf2" />

5. **Pipeline de IngestÃ£o**
   - Execute o notebook `Driver` para:
     - Transformar dados da camada Bronze em Silver
     - Criar tabelas Delta no Hive Metastore

6. **ExecuÃ§Ã£o das AnÃ¡lises**
   - Navegue atÃ© `analysis/` no Repos
   - Execute:
     - `q1_avg_monthly_total_amount.sql`
     - `q2_avg_passengers_per_hour_may.sql`

## ğŸ“Š AnÃ¡lises Realizadas
As queries respondem Ã s seguintes perguntas:

1. **MÃ©dia de valor total mensal (`total_amount`)** das corridas realizadas por yellow taxis  
2. **MÃ©dia de passageiros (`passenger_count`) por hora do dia** no mÃªs de maio considerando todas as corridas

---

ğŸš€ Para dÃºvidas, contribuiÃ§Ãµes ou sugestÃµes, fique Ã  vontade para abrir uma issue ou pull request!
